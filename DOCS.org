#+TITLE: Take Home Challenge
#+SETUPFILE: ~/s3sync/org/conf/setup.config
#+FILETAGS: :recruiting:tiko

* Prerequisites
- Python (anaconda3-2022.05)

* Design
#+CAPTION: general-approach
#+attr_html: :width 100%

[[file:docs/general-approach.drawio.png][file:./docs/general-approach.drawio.png]]

* Approach
** Exercise 1: K8s
- Review Kind and run the deployment
- Fix the issue regarding control-plane connection (I simply changed the port in
  =kubeconfig= file to match the =tiko-control-plane= docker port (could have been
  done the other way around as well)
- Create a "project design". This is probably not needed for a smaller exercise,
  but this would be my approach especially with regards to network relevant
  dependencies. I.e. hybrid- or multi-cluster setups.
- ..

*** 1. Sub-Task: Troubleshooting
**** Fixing runner deployment
- After apps deployment, only =prometheus= is up
#+begin_src bash
$ helm --kubeconfig infra/kubeconfig list --all --all-namespaces
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION
prometheus      monitoring      1               2022-07-13 19:04:10.820234091 +0000 UTC deployed        kube-prometheus-stack-37.2.0    0.57.0
#+end_src

- running manually did not pose a problem (release wise)
#+begin_src bash
$ helm --kubeconfig ./infra/kubeconfig install runner ./apps/charts/runner --namespace runner --create-namespace
NAME: runner
LAST DEPLOYED: Thu Jul 14 11:41:47 2022
NAMESPACE: runner
STATUS: deployed
REVISION: 1
TEST SUITE: None

$ helm --kubeconfig infra/kubeconfig list --failed
NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION

$ helm --kubeconfig infra/kubeconfig ls --all --all-namespaces
NAME            NAMESPACE       REVISION        UPDATED                                         STATUS          CHART                           APP VERSION
prometheus      monitoring      1               2022-07-13 19:04:10.820234091 +0000 UTC         deployed        kube-prometheus-stack-37.2.0    0.57.0     
runner          runner          1               2022-07-14 11:41:47.746516074 +0200 CEST        deployed        runner-0.1.0                    1.16.0     

$ helm --kubeconfig infra/kubeconfig ls --all --all-namespaces
NAME            NAMESPACE       REVISION        UPDATED                                         STATUS          CHART                           APP VERSION
prometheus      monitoring      1               2022-07-13 19:04:10.820234091 +0000 UTC         deployed        kube-prometheus-stack-37.2.0    0.57.0     
runner          runner          1               2022-07-14 11:41:47.746516074 +0200 CEST        deployed        runner-0.1.0                    1.16.0     
#+end_src

- runner ends up in =CrashLoop=
#+begin_src bash
$ kubectl --kubeconfig infra/kubeconfig get all -n runner
NAME                          READY   STATUS             RESTARTS      AGE
pod/runner-5b4b5ddf49-67m6j   0/1     CrashLoopBackOff   4 (21s ago)   2m19s

NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/runner   0/1     1            0           2m19s

NAME                                DESIRED   CURRENT   READY   AGE
replicaset.apps/runner-5b4b5ddf49   1         1         0       2m19s
#+end_src

- I've first looked into the pod and the deployment details for finding an error
- As there was no significant error cause identifiable, I thought about looking
  for obvious syntax error in the runner Helm chart definition files
- The pod itself does nothing besides running a bash command. The successful
  bash command simply returns working, and only does so when a certain env
  variable is checked.
- As this variable was missing, I've hardcoded it in the very same deployment
  definition file. And the pod was running its command as expected.

**** Nginx-Ingress
- According to my best knowledge, I've recognized that there is no service
  definition inside the Helm chart for the web app.
- About the ingress, I am not sure as I have 2 major concerns (that might not
  be true):
  1. The ingress-controller is in a different namespace than the web service
  2. An additional ingress should be deployed in the same namespace exposing the
     web service in particular

***** Current FAQs (stuck)
- In order to understand the service better, i compared running against Helm
  template definitions.
- Of concern was that I was looking for the selector labels set correctly:
  #+begin_src bash
apiVersion: v1
kind: Service
...
selector:
    app.kubernetes.io/instance: web
    app.kubernetes.io/name: web
  #+end_src

- Both of those can also be found in the live deployment definition:
#+begin_src bash
apiVersion: apps/v1
kind: Deployment
...
  labels:
    app.kubernetes.io/instance: web
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: web
    app.kubernetes.io/version: 1.16.0
    helm.sh/chart: web-0.1.0
#+end_src

***** My next steps
- I would like to directly go into a cluster node (here: docker) and try to
  reach the service endpoint on its internal Cluster-IP
- .. I tried but neither succeeded with reaching the Cluster-IP, nor was able to
  run a simple curl-pod within the same namespace
- .. I would need to understand KinD better, as both of the above methods should
  not pose a problem.

*** Remarks
- I haven't been using _kind_ before. My local clusters where in [[https://minikube.sigs.k8s.io/docs/][minikube]], [[https://k3s.io/][k3s.io]],
  and [[https://rancher.com/docs/rke/latest/en/][rke]].
- From all the solutions RKE offers the possibility to run on Vagrant and in
  production the exact same way. I think this (pretty transparent) deployment
  pattern got neglected in the Kubernetes community a lot. I.e try running EKS
  locally???
- So far, I already am stuck with the first relevant web service task. The major
  problems I have was to simply deploy stuff on KinD and also the use of Helm
  charts as an additional abstraction layer (no deployment logs or errors
  visible, walking in the dark..)
- A usual approach in working with a Cluster would be to first deploy a cluster
  with the given technology (KinD) myself.
- Handling a simple nginx would be the first task after cluster is understood
- I would not use Helm as I see it more as a technology to really harden well
  tested and well running applications.


** Exercise 2: Programming
- Inspect the code (all code bases)
- Run functions, Js in browser (i.e. using log.console)
- Use IDE to jump into function definitions (either docs or jumping to package sources)
- Explain the logic in simple words

** Exercise 3: Terrafom
- Searching for the appropriate terraform module: [[https://registry.terraform.io/modules/terraform-aws-modules/s3-bucket/aws/3.3.0][terraform-aws-modules/s3-bucket/aws/3.3.0]]
- Loading module inside main with simple hardcoded values and smoke-test
  deployment in own account
- Create a map variable that holds the dynamic values, smoke-test with terraform
  plan => there should be no changes if it is correct
- Create a =locals= function that is creating the map dynamically from a given
  list input, again try with hard-coded list in =terraform.tfvars=, so that there
  are no changes in terraform plan (inpect map output)
- Finally, I've decided to use Python to create a =terraform.tfvars.json=

#+begin_src bash
$ python scripts/generate_tfvars.py -h
$ python scripts/generate_tfvars.py 100
#+end_src

*** Remarks
- Usually, in my own projects I have a slightly different approach.
- I've build my own terraform wrapper in Golang. This wrapper is CI ready and
  offers the possibility to inject =*.tfvars= files dynamically as well
- In Golang (but I am sure that this is possible with Python as well) I am using
  the =text/template= package for generating hcl template files. This is slightly more
  involved but the extra step in having a file, helps to review if necessary.
- I am using validate linters for hcl, so terraform will not even run if the
  files do not come out in the proper format. Also, it allows for writing tests.
- Next to that, it is obviously needed to provide some kind of secret
  management. Currently I am using my wrappertool and [[https://www.gopass.pw/][https://www.gopass.pw/]].
- Ultimately, this process could be replaced with a production Hashicorp Vault
  setup (especially for CI)
- Terraform now offers a =pass-store= resource that is also using =Gopass=.
- For the sake of demonstration and your exercise, I decided to build a new
  Python script for generating the variables, instead of using my own tools.
- If you are interested I am happy to demonstrate them.

* General Instructions
** Sub-topic 1
- ..

* Author
Olaf Marangone
Contact: olaf.marangone@gmail.com

* Credits
- ..

* More sources
- ..

* Where to go from here?
- ..

